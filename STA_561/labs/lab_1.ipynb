{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd89756a",
   "metadata": {},
   "source": [
    "# Lab 1: Ridge and Lasso with CV, GCV, AIC, and BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b9c66f",
   "metadata": {},
   "source": [
    "The aim of this lab is to review two examples of penalized regression--namely ridge and lasso. Both of these approaches have a hyperparameter or penalty parameter that needs to be specified. There are many ways to do this, and we will cover some examples, including CV, GCV, AIC, and BIC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7540c4",
   "metadata": {},
   "source": [
    "*This lab is adapted from a previous STA 561 lab written by Ed Tam.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f7b6e7",
   "metadata": {},
   "source": [
    "## Review of Ridge and Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12460c5e",
   "metadata": {},
   "source": [
    "Recall from class that the OLS estimator is an unbiased estimator that attains the minimum variance among all unbiased estimators. It may seem that these are good properties to have, which begs the question why we would need to introduce other forms of regression. It turns out penalizing the regression problem can lower overall MSE below at the cost of introducing bias. Two ways we saw in class to do this are ridge regression\n",
    "$$\n",
    "\\widehat{\\mathbf{\\beta}}_n^\\lambda := \\underset{\\mathbf{\\beta}}{\\arg\\min}\\; \\mathbb{P}_n(Y - \\mathbf{X}^\\intercal\\mathbf{\\beta})^2 + \\lambda\\|\\mathbf{\\beta}\\|_2^2\n",
    "$$\n",
    "and lasso regression\n",
    "$$\n",
    "\\widehat{\\mathbf{\\beta}}_n^\\tau := \\underset{\\mathbf{\\beta}}{\\arg\\min}\\; \\mathbb{P}_n(Y - \\mathbf{X}^\\intercal\\mathbf{\\beta})^2 + \\tau\\|\\mathbf{\\beta}\\|_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665961e7",
   "metadata": {},
   "source": [
    "**Question 1:** Why would you opt to use ridge over lasso? Lasso over ridge?\n",
    "\n",
    "**Question 2:** (Optional) Are there penalized regression methods that you could use? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1037b385",
   "metadata": {},
   "source": [
    "### Estimating $\\mathbf{\\beta}$ with ridge and lasso in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11c3ec5",
   "metadata": {},
   "source": [
    "For today's lab, we will be using the `diabetes` dataset for analysis. Let's start by loading the data and splitting it into train and test sets. Don't forget to scale the design matrix for train and test (why?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7af3756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "y = diabetes.target\n",
    "X = diabetes.data\n",
    "\n",
    "# Split data into train and test 75/25\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Standardize the design matrix for train and test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30355c7b",
   "metadata": {},
   "source": [
    "Using the train data, fit a linear model (without any penalty terms). Predict using the test data and save the results on `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a6549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Fit OLS model here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a99a38",
   "metadata": {},
   "source": [
    "Using the train data, fit a ridge and lasso regression (using your favorite penalty parameter). Predict the test data and save the results as `y_ridge` `y_lasso`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee670a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "\n",
    "# Fit ridge regression\n",
    "\n",
    "\n",
    "# Fit lasso regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0e6f6f",
   "metadata": {},
   "source": [
    "### Assessing Model Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e685752",
   "metadata": {},
   "source": [
    "Next, let's compare how the models perform.\n",
    "\n",
    "**Question 3:** Compute the MSE and $R^2$ for the test data. Make a table and/or plot for the coefficients comparing them across the models. Describe your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c476d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code goes here\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01027ef1",
   "metadata": {},
   "source": [
    "**Question 4:** (Optional Python practice) Write a function that takes a vector of penalty parameters, training data, and test data, and then performs ridge and lasso regression for each penalty parameter. The output should include a vector of MSE values, a vector of $R^2$ values, and an array of coefficients. Assess what penalty parameters work well for each model, and what behavior you observe with the coefficients as the penalty parameter varies. What is one challenging conceptual aspect of this exercise?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6354aaa",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff1178d",
   "metadata": {},
   "source": [
    "If you did Question 4 (or recall from class), choosing penalty parameters can be tricky. Fortunately, `sklearn` has a lot of built-in tools for tuning these hyperparameters. We'll cover 4: CV, GCV, AIC, and BIC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bb79eb",
   "metadata": {},
   "source": [
    "### CV and GCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1466a6",
   "metadata": {},
   "source": [
    "The idea behind $k$-fold cross-validation (CV) is to partition the data randomly into $k$ groups of data points. We then train on the remaining data, and compute the error on the left out goruop. We then do this across all of the elements in the partition, and average the error. This gives us an idea of the error associated to using a certain value for the penalty parameter $\\lambda$. We can compute the the CV error across different values of penalty parameters to choose a regression model.\n",
    "\n",
    "**Question 5:** How is LOOCV related to CV? Do we randomly partition the data?\n",
    "\n",
    "Generalized cross-validation (GCV) is an approximation to CV that is motivated by the fact that some problems are rotation-invariant. In both CV and GCV, we want to have low cross-validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f5f442",
   "metadata": {},
   "source": [
    "### AIC and BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a905f028",
   "metadata": {},
   "source": [
    "The Aikake Information Criterion (AIC) and the Bayesian Information Criterion (BIC) choose a model that fits the data well while penalizing complexity via the number of parameters. More formally,\n",
    "$$\n",
    "\\text{AIC} := -2\\log(\\widehat{L}) + 2p\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\text{BIC} := -2\\log(\\widehat{L}) + \\log(n)\\cdot p\n",
    "$$\n",
    "where $\\widehat{L}$ is the maximum likelihood of the model.\n",
    "\n",
    "Regression models with smaller AIC/BIC are preferred.\n",
    "\n",
    "**Question 6:** What can you say about the size of models that AIC will prefer relative to BIC?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732d7629",
   "metadata": {},
   "source": [
    "### Model Selection with CV, GCV, AIC, and BIC in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f80d856",
   "metadata": {},
   "source": [
    "We will continue using the `diabetes` dataset as above. We'll use a method called LARS (least angle regression) for model selection with Lasso. The nice benefit of this is that it automatically computes the lasso path for us.\n",
    "\n",
    "Start by fitting 10-fold cross-validation on the training data set using Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba561bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "### This code borrows from the following useful sklearn documentation: https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html#sphx-glr-auto-examples-linear-model-plot-lasso-model-selection-py ###\n",
    "\n",
    "# Load package\n",
    "from sklearn.linear_model import LassoLarsCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Run 10-fold CV\n",
    "lasso = LassoLarsCV(cv=10).fit(X_train, y_train)\n",
    "\n",
    "# Show CV path\n",
    "plt.semilogx(lasso.cv_alphas_, lasso.mse_path_, \":\")\n",
    "plt.semilogx(\n",
    "    lasso.cv_alphas_,\n",
    "    lasso.mse_path_.mean(axis=-1),\n",
    "    color=\"black\",\n",
    "    label=\"Average across the folds\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.axvline(lasso.alpha_, linestyle=\"--\", color=\"black\", label=\"alpha CV\")\n",
    "\n",
    "ymin, ymax = 2300, 3800\n",
    "plt.ylim(ymin, ymax)\n",
    "plt.xlabel(r\"$\\alpha$\")\n",
    "plt.ylabel(\"Mean square error\")\n",
    "plt.legend()\n",
    "_ = plt.title(f\"Mean square error on each fold: Lars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1365640",
   "metadata": {},
   "source": [
    "Next, let's use AIC and BIC to choose $\\tau$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ba3044",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This code borrows from the following useful sklearn documentation: https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html#sphx-glr-auto-examples-linear-model-plot-lasso-model-selection-py ###\n",
    "\n",
    "\n",
    "#Load package\n",
    "from sklearn.linear_model import LassoLarsIC\n",
    "import pandas as pd\n",
    "\n",
    "# Fit the model using the AIC criterion\n",
    "lasso_lars_aic = LassoLarsIC(criterion=\"aic\")\n",
    "lasso_lars_aic.fit(X_train, y_train)\n",
    "alpha_aic = lasso_lars_aic.alpha_\n",
    "\n",
    "# Fit the model using the BIC criterion\n",
    "lasso_lars_bic = LassoLarsIC(criterion=\"bic\")\n",
    "lasso_lars_bic.fit(X_train, y_train)\n",
    "alpha_bic = lasso_lars_bic.alpha_\n",
    "\n",
    "# Store results in a dataframe for plotting\n",
    "results = pd.DataFrame(\n",
    "    {\n",
    "        \"alphas\": lasso_lars_aic.alphas_,\n",
    "        \"AIC criterion\": lasso_lars_aic.criterion_,\n",
    "    }\n",
    ").set_index(\"alphas\")\n",
    "results[\"BIC criterion\"] = lasso_lars_bic.criterion_\n",
    "\n",
    "# Plot output\n",
    "ax = results.plot()\n",
    "ax.axvline(alpha_aic, linestyle=\"--\", color=\"blue\", label=\"alpha CV\")\n",
    "ax.axvline(alpha_bic, linestyle=\"--\", color=\"orange\", label=\"alpha CV\")\n",
    "ax.set_xlabel(r\"$\\alpha$\")\n",
    "ax.set_ylabel(\"criterion\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.legend()\n",
    "_ = ax.set_title(\n",
    "    f\"Information-criterion for model selection\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77a7e3f",
   "metadata": {},
   "source": [
    "**Question 7:** How do the values of $\\tau$ compare across the different criteria?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca61add",
   "metadata": {},
   "source": [
    "### What about CV with Ridge?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7384b80e",
   "metadata": {},
   "source": [
    "Good news! If you run LOOCV with ridge regression `sklearn`, it automatically runs GCV. Try it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cfec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load package\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# Fit ridge regression using LOOCV\n",
    "alphas = np.logspace(-2, -0.1, 30)\n",
    "ridge_cv = RidgeCV(alphas = alphas, store_cv_values = True).fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4ab7c0",
   "metadata": {},
   "source": [
    "**Question 8:** Now make a plot of the MSE (use `ridge_cv.cv_values_` to access these) across some folds. Plot alongside it the average across all of the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce23b260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plot of averaged LOOCV error over folds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f38012",
   "metadata": {},
   "source": [
    "### Optional Extension: Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffc0d48",
   "metadata": {},
   "source": [
    "Recall Question 2. A very simple extension to the ideas covered here is a penalization method that penalizes using *both*  an $\\ell_2$ and an $\\ell_1$ term. This can be helpful to leverage the advantages of both regularization methods (reducing collinearity and inducing sparsity). The regression problem is given by\n",
    "$$\n",
    "\\widehat{\\mathbf{\\beta}}_n^{\\lambda,\\tau} := \\underset{\\mathbf{\\beta}}{\\arg\\min}\\; \\mathbb{P}_n(Y - \\mathbf{X}^\\intercal\\mathbf{\\beta})^2 + \\lambda\\|\\mathbf{\\beta}\\|_2^2 + \\tau\\|\\mathbf{\\beta}\\|_1\n",
    "$$\n",
    "\n",
    "Note that $\\lambda$ and $\\tau$ here are not necessarily the same parameters as when you would tune ridge or lasso separately. In fact, in `sklearn`, you only pass one parameter: `l1_ratio`, which is a number between 0 and 1 that corresponds to the ratio of the penalty parameters in elastic net. A value of 1 reduces to Lasso regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaedd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This section borrows on some ideas from this documentation: https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_and_elasticnet.html#sphx-glr-auto-examples-linear-model-plot-lasso-and-elasticnet-py ###\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "enet = ElasticNet(l1_ratio=0.7)\n",
    "enet.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb07162",
   "metadata": {},
   "source": [
    "**Question 9:** Using the output of `enet`, find the MSE, $R^2$, and the values of the coefficients. Then compare the results to ridge and lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81683fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
